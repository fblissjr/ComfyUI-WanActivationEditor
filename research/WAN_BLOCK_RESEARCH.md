# **An Architectural and Functional Analysis of Transformer Blocks in the WanVideo 2.1 Generative Model for Advanced Control**
**Date:** 2025-07-24

## **Section 1: Architectural Deconstruction of the WanVideo 2.1 Diffusion Transformer**

### **1.1 The WanVideo Generative Pipeline: A High-Level Overview**

The WanVideo 2.1 suite of models, developed by Alibaba, represents a significant advancement in open-source video generation, built upon the mainstream Diffusion Transformer (DiT) paradigm.1 To comprehend the functional role of its internal components, it is essential to first understand the high-level data flow through its generative pipeline. This pipeline can be deconstructed into three primary stages: Encoding, Latent Denoising, and Decoding. Each stage leverages specialized components designed to handle the high dimensionality and temporal complexity of video data.

Stage 1: Encoding
The initial stage of the pipeline is responsible for converting high-dimensional input data—such as text prompts, images, or videos—into a compressed, lower-dimensional latent representation. This is a critical step for computational tractability, as training a diffusion model directly in pixel space is prohibitively expensive.4

* **Spatio-Temporal Compression (Wan-VAE):** A key innovation within WanVideo is its novel 3D causal spatio-temporal Variational Autoencoder, termed Wan-VAE.5 This component is specifically designed for video, improving spatio-temporal compression, reducing memory usage, and ensuring temporal causality.3 It demonstrates exceptional efficiency, capable of encoding and decoding videos of unlimited length at resolutions up to 1080p while preserving temporal information, making it a robust foundation for the subsequent generative process.10 The Wan-VAE's reconstruction speed, under equivalent hardware, is reportedly 2.5 times faster than competing methods, an advantage that scales with higher resolutions due to its efficient design and feature caching mechanism.9 For Image-to-Video (I2V) tasks, the condition image is concatenated with zero-filled frames and then compressed by the Wan-VAE into a condition latent representation.2
* **Textual Conditioning (UMT5-XXL Encoder):** The model interprets textual instructions through a UMT5-XXL (Text-To-Text Transfer Transformer) Encoder, specifically the UMUMT5-XXL variant.2 This component processes multilingual text inputs (supporting both English and Chinese) and transforms them into high-dimensional vector embeddings.1 These embeddings serve as the primary conditioning signal that guides the generative process, dictating the semantic content of the output video.14

Stage 2: Latent Denoising (The Core Generative Process)
This is the heart of the WanVideo model. The process begins with a tensor of random noise in the latent space, which is iteratively refined over a series of timesteps to produce a structured latent representation that corresponds to the final video.16 This reverse diffusion process is governed by a powerful Diffusion Transformer (DiT), which predicts and removes the noise at each step based on the conditioning signals provided.2 The entire process is built upon a Flow Matching framework, which contributes to more stable training and faster inference compared to conventional diffusion approaches.2
Stage 3: Decoding
In the final stage, the fully denoised latent tensor, which now contains the complete, structured information for the video, is passed through the decoder component of the Wan-VAE.5 The decoder reconstructs the latent representation back into the pixel space, generating the sequence of high-resolution video frames that form the final output. The efficiency and fidelity of the Wan-VAE are paramount in this stage to ensure that the details and temporal coherence crafted in the latent space are accurately translated into the final video product.

### **1.2 Anatomy of the 14B Model's Diffusion Transformer (DiT)**

The core DiT module of the WanVideo 2.1 14-billion parameter model is a deep stack of transformer layers, or blocks, that collaboratively execute the denoising process. Understanding its specific architecture is fundamental to analyzing the effects of block-level interventions.

* **The 40-Block Stack:** Architectural specifications reveal that the WanVideo 2.1 14B model's DiT is composed of 40 distinct transformer layers.6 Community experimentation within tools like ComfyUI, particularly through memory management nodes like
  BlockSwap, corroborates this 40-block structure.20 This number provides a concrete and essential map for analyzing and targeting specific regions of the model's computational hierarchy.
* **Internal Block Structure:** Each of these 40 blocks adheres to the standard transformer architecture, containing a self-attention mechanism, a cross-attention mechanism, and feed-forward neural networks.2 The self-attention mechanism allows different patches of the latent video to communicate with each other, establishing spatio-temporal relationships across frames.4 The cross-attention mechanism is the primary port for external guidance, as detailed below.
* **Conditioning Injection via Cross-Attention:** A critical architectural choice in WanVideo is how conditioning information is integrated. The text embeddings from the UMT5-XXL encoder, along with time embeddings that inform the model of its position in the denoising schedule, are injected into *each of the 40 transformer blocks* via a dedicated cross-attention mechanism.2 This continuous injection of guidance throughout the entire depth of the network is a profound design decision. It means the model is not merely given an initial instruction and left to execute; rather, it is constantly reminded of the user's prompt and its temporal context at every single stage of processing. This architecture renders the model highly receptive to mid-process interventions, as each block is natively designed to "listen for" and process conditioning signals. Therefore, the user's hypothetical query about injecting a new text embedding at a specific block is not a matter of hacking an unrelated process but of hijacking a built-in, pervasive mechanism. This makes targeted manipulation not only feasible but architecturally logical.

## **Section 2: Functional Stratification of Transformer Blocks**

While the 40 transformer blocks in the WanVideo 14B model are structurally similar, there is substantial theoretical and empirical evidence to suggest they are not functionally monolithic. Instead, they form a processing hierarchy where different tiers of blocks specialize in distinct aspects of the video generation task, progressing from coarse compositional structure to fine-grained stylistic detail.

### **2.1 Theoretical Underpinnings: Hierarchical Feature Learning in Vision Transformers**

The concept of functional stratification in deep neural networks is well-established. In convolutional neural networks (CNNs), early layers learn to detect low-level features like edges and textures, while deeper layers combine these to recognize more complex, high-level semantic concepts such as objects and entire scenes.23 This principle of hierarchical feature learning extends directly to Vision Transformer (ViT) and Diffusion Transformer (DiT) architectures.4

In the context of a DiT like WanVideo, this hierarchy manifests during the iterative denoising process. The initial blocks operate on a latent space that is still highly noisy and unstructured, and their primary role is to establish the most fundamental, low-frequency information of the target video—its overall composition and primary motion vectors. As the denoising progresses and the latent representation becomes more structured, subsequent blocks can focus on refining higher-frequency information, moving from broad semantics to intricate details.

### **2.2 Empirical Evidence from Community Experimentation**

The theoretical model of hierarchical function is strongly corroborated by a wealth of empirical evidence from the user community. Through tools like ComfyUI, advanced users are effectively conducting large-scale, distributed research into the model's internal mechanics, primarily through two techniques: LoRA block weighting and block skipping/disabling.

* **Evidence from LoRA Block Weighting:** Low-Rank Adaptation (LoRA) allows for fine-tuning a model by injecting small, trainable matrices into its layers.30 Advanced tools, such as the
  LoraLoaderBlockWeight node in the ComfyUI Inspire Pack, allow users to specify the strength of the LoRA's influence on a block-by-block basis.32 A consistent finding from these experiments is that the function of the LoRA is heavily dependent on which blocks it is applied to. One user explicitly notes that, based on their experiments, "the first layers have something to do with composition, while the last ones have something to do with details... in the context of a video, lora, it would be like the first few blocks affect motion".34 Another experiment focusing on a face LoRA demonstrated that applying weights only to the first and twentieth blocks (
  1,0,0,...,1) was sufficient to capture the character's facial identity without fully imposing the LoRA's overall style, suggesting a separation of identity and stylistic information across the block stack.35
* **Evidence from Block Disabling:** A highly informative set of experiments involves completely disabling a range of blocks. One user reported that when using a LoRA designed to control a character's pose, disabling blocks 20 through 39 significantly improved the output.34 With the blocks active, the LoRA's effect was compromised, making the video "overly blurry or changing the way I want it to look." With the later blocks disabled, the intended pose was preserved. This provides powerful evidence that these later blocks are responsible for applying a layer of detail and stylistic finish that can interfere with or "overwrite" the more fundamental instructions provided by a LoRA or the earlier blocks. This finding was even corroborated by an LLM analysis, which suggested that blocks 20-39 are used to "add small detail".36
* **Evidence from Skip Layer Guidance (SLG):** As will be discussed in greater detail in Section 3, the community has discovered that skipping specific, intermediate blocks during the unconditional guidance pass can dramatically improve video quality. The most frequently targeted block for this technique is block 9\.37 This intense focus on a single block within the early-to-mid range of the stack suggests it plays a uniquely critical role in processing and solidifying the semantic content of the video.

### **2.3 A Proposed Functional Map of the WanVideo 14B Transformer Blocks**

Synthesizing the theoretical principles of hierarchical learning with the empirical evidence from community experimentation allows for the construction of a functional map for the WanVideo 14B model's 40 transformer blocks. This map can be divided into three primary tiers.

* **Tier 1: Compositional & Motion Blocks (Approx. 1-7):** These initial blocks operate on the most chaotic latent states. Their primary function is to establish the fundamental, low-frequency information of the video: the overall scene layout, camera perspective, subject placement, and the macro-level vectors of motion. Interventions at this stage would likely cause drastic changes to the entire video's structure.
* **Tier 2: Semantic & Conceptual Blocks (Approx. 8-19):** This middle tier is responsible for refining the semantic meaning of the video. Once the basic composition is set, these blocks work to define and solidify subject identity, interpret complex actions described in the prompt, and ensure conceptual coherence between different elements. This is the tier where the model transitions from understanding "what the scene looks like" to "what the scene *is*." The user's query regarding blocks 8 and 9 correctly identifies this as the model's semantic core.
* **Tier 3: Detail & Stylistic Blocks (Approx. 20-40):** These final blocks act as the "finishers." They take the semantically coherent structure established by the earlier tiers and apply high-frequency details, realistic textures, complex lighting effects, and the overall aesthetic style. They are less concerned with what the subjects are and more with how they are rendered.

This proposed functional hierarchy is summarized in the table below, providing a clear reference for advanced control and experimentation.

**Table 1: Hypothesized Functional Map of WanVideo 14B Transformer Blocks**

| Block Range (Approx.) | Tier Name | Hypothesized Primary Function | Key Control Mechanisms & Community Findings | Supporting Evidence |
| :---- | :---- | :---- | :---- | :---- |
| 1-7 | Compositional | Establishes scene layout, camera perspective, and macro-level motion. | LoRA block weighting on early blocks affects overall composition and motion. | 34 |
| 8-19 | Semantic | Defines and refines subject identity, interprets actions, ensures conceptual coherence. | **Block 9** is a critical coherence checkpoint; skipping it via SLG improves quality. This tier is the ideal target for semantic interventions like text embedding injection. | 37 |
| 20-40 | Stylistic / Detail | Adds high-frequency details, textures, lighting, color grading, and stylistic finish. | Disabling blocks 20-39 improves LoRA subject adherence by preventing the model's native detail generation from "overwriting" the LoRA's specific training. | 34 |

## **Section 3: Analysis of Targeted Block-Level Interventions and Their Effects**

The functional stratification of the transformer stack provides a powerful predictive framework for understanding the impact of direct, targeted interventions. By manipulating specific blocks, a user can exert fine-grained control over distinct aspects of the generative process, from the core meaning of the video to its final aesthetic polish. This section addresses the user's specific query and examines two prominent community-developed techniques as case studies.

### **3.1 Direct Response to Query: The Impact of Text Embedding Injection at Blocks 8 and 9**

The user's query—"if i inject a text embed into block 8,9 \- what does that do to the end video result?"—targets the heart of the model's semantic processing tier. As established, the cross-attention mechanism within each block is designed to integrate conditioning signals. Injecting a new or modified text embedding at this stage would effectively perform a "semantic transplant" mid-generation, directly altering the instructions the model uses to define the video's content.

* **Hypothesized Mechanism:** At steps corresponding to blocks 8 and 9, the denoising process has moved past initial composition and is actively solidifying what the subjects of the video *are* and what they are *doing*. The model is resolving ambiguity from the noisy latent state into concrete concepts. Intervening here with a new text embedding forces the model to reconcile its current trajectory with a new, potentially conflicting, semantic goal. The model must then attempt to alter the latent representation to match this new instruction within the remaining \~30 blocks of processing.
* **Predicted Outcomes:**
  * **Conceptual Morphing:** This is the most likely outcome if the injected embedding represents a different subject than the original prompt (e.g., injecting "a cat" into a generation for "a dog"). The intervention occurs too late to cleanly re-establish the scene's composition around the new subject but early enough to fundamentally alter its identity. The result would likely be a video where a dog-like form abruptly or fluidly transforms into a cat, or a chimeric creature possessing features of both.
  * **Action Hijacking:** If the injected embedding contains a different action (e.g., injecting "jumping" into a prompt for "walking"), the model would be forced to apply this new motion command to the already-established subject and trajectory. This could result in a sudden, physically jarring change in movement, as the model attempts to pivot from a walking motion to a jumping motion without the benefit of the initial compositional setup that a "jumping" prompt would have created from block 1\.
  * **Enhanced Adherence or Overfitting:** Re-injecting the *same* text embedding at this stage could serve as a powerful corrective measure. If the model has begun to "drift" from the prompt's intent, this re-injection would force it to re-evaluate its current latent state against the core semantic instruction. This could be a technique to improve prompt adherence, but it also carries the risk of creating an overly literal or "over-baked" result, where the model's interpretation becomes rigid and loses nuance.

### **3.2 Investigative Case Study 1: Skip Layer Guidance (SLG) on Block 9**

Skip Layer Guidance (SLG) is a powerful, training-free sampling technique that has been empirically shown to enhance the quality of transformer-based video diffusion models.39 Its application within the WanVideo community provides a compelling case study into the function of the semantic block tier.

* **The Theory of Perturbed Unconditional Guidance:** The foundational technique for guiding diffusion models is Classifier-Free Guidance (CFG). During each denoising step, the model performs two forward passes: a *conditional* pass guided by the text prompt and an *unconditional* pass, typically guided by an empty or negative prompt. The final denoising direction is an extrapolation away from the unconditional prediction and towards the conditional one.37 The community's working theory behind SLG is that one can improve this process by intentionally degrading the unconditional prediction. By skipping a layer during the unconditional pass, a "worse" or less coherent unconditional latent is created. When this degraded latent is subtracted from the conditional one, the resulting vector points more strongly towards the desired features, thus enhancing the final quality.37
* **Observed Results:** Community members using ComfyUI have widely reported that adding a SkipLayerGuidanceDiT node after the model loader and configuring it to skip block 9 (and sometimes adjacent blocks like 7, 8, or 10\) results in a dramatic improvement in video quality.37 The effect is particularly satisfying for generations involving human subjects, leading to better motion and fewer artifacts.37
* **Inferred Function of Block 9:** The effectiveness of SLG on block 9 strongly supports the hypothesis that this block serves as a crucial **"coherence checkpoint"** within the semantic tier. During a standard unconditional pass, the denoising process might begin to form random, undesirable patterns or "proto-subjects" from the noise. Block 9 appears to be a stage where these patterns are significantly amplified or refined. By skipping this block in the unconditional pass, the latent representation is prevented from coalescing into these unwanted coherent structures, remaining more purely noisy. This provides a cleaner, more neutral baseline for the CFG process, allowing the features from the conditional pass to be expressed with greater clarity and fidelity.

### **3.3 Investigative Case Study 2: Disabling Late Blocks (20-39) for LoRA Adherence**

The practice of selectively disabling the final tier of transformer blocks when using LoRAs reveals a fundamental tension between the specialized knowledge of a fine-tuned adapter and the generalized knowledge of the base model.

* **The Conflict Between LoRA Identity and Model-Native Detail:** A LoRA is a fine-tuning method that adapts a small subset of a model's weights to learn a new, specific concept, such as a particular character's face or a unique artistic style.30 The base WanVideo model, however, has been trained on billions of images and videos, and its later blocks (Tier 3\) are highly adept at applying generic, high-frequency details to create a realistic or aesthetically pleasing final image.1 This creates a conflict: the LoRA provides a specific identity or style, but the later blocks, following their original training, attempt to apply their own generic detailing, which can dilute, distort, or "overwrite" the LoRA's intended effect.34
* **Visual Analysis of Outputs:** User reports describe the outcome of this conflict clearly. When using a character or pose LoRA with all 40 blocks active, the final video can become "overly blurry" or the character's appearance changes, losing fidelity to the LoRA's training.34 However, by using a block editing node to disable blocks 20-39, users find that the core subject and pose from the LoRA are preserved with much higher accuracy.
* **Inferred Function of Blocks 20-39:** This experiment confirms that these later blocks function as the **"Detail and Style"** layers. Their primary role is not to define the core content—which has already been established by the earlier semantic blocks—but to execute the final rendering. Disabling them is a strategic trade-off: one gains fidelity to the LoRA's specific knowledge at the expense of the base model's powerful and generalized detailing capabilities. The resulting video may be more true to the LoRA but potentially less detailed or realistic than a video generated with the full 40-block stack.

These distinct intervention strategies—semantic injection, guidance perturbation, and detail layer disabling—are not merely isolated tricks. They represent a unified spectrum of control that maps directly onto the hierarchical function of the transformer stack. An advanced practitioner can thus select an intervention point based on a desired outcome: manipulate the **Semantic** tier (blocks \~8-19) to alter the video's core meaning; manipulate the **Guidance Signal** at this tier to improve quality and coherence; or manipulate the **Detail** tier (blocks \~20-40) to arbitrate the balance between a fine-tuned adapter and the base model's native style. This provides a complete and actionable mental model for achieving precise, predictable control over the WanVideo generative process.

## **Section 4: A Practical Framework for Experimental Block Manipulation**

The theoretical and empirical analysis of WanVideo's block functions can be translated into a practical framework for experimentation and control. Using the node-based interface of ComfyUI, users can directly probe and manipulate the transformer stack to achieve specific artistic and technical outcomes.

### **4.1 Methodologies for Probing Block Functions in ComfyUI**

Conducting systematic research into block functions requires a specific set of tools and a structured experimental setup.

* **Required Tooling:** A robust ComfyUI environment for these experiments should include several key custom node packages:
  * **ComfyUI-WanVideoWrapper:** Developed by Kijai, this wrapper is essential for running WanVideo models and provides access to advanced features and memory management tools like BlockSwap.42
  * **ComfyUI-Inspire-Pack:** This package is critical for its LoraLoaderBlockWeight node, which allows for the application of LoRAs with per-block strength values.32 It also includes the
    LoraBlockInfo node, which can be used to inspect the structure of a given LoRA.45
  * **Native SkipLayerGuidanceDiT Node:** This node is typically included in up-to-date versions of ComfyUI and is the primary tool for implementing SLG.38
* **Experimental Setup:** To systematically map the function of each block, an XY plot workflow is highly effective. Using a node like XY Input: Lora Block Weight from the Inspire Pack, a user can create a grid of images where the X-axis iterates through different LoRAs (e.g., a style LoRA, a character LoRA, a motion LoRA) and the Y-axis iterates through block weight vectors. For example, a Y-axis could test vectors like 1,0,0,..., 0,1,0,..., 0,0,1,..., etc., to isolate the effect of applying the LoRA to each individual block. This allows for a direct visual analysis of which blocks are most influential for style, identity, or motion. Similarly, the double\_layers and single\_layers parameters of the SkipLayerGuidanceDiT node can be varied to test the impact of skipping different blocks on video coherence and quality.47

### **4.2 Recipes for Advanced Artistic and Technical Control**

The analytical findings of this report can be distilled into practical "recipes" for achieving specific, predictable results in video generation.

* **Recipe 1: Subject/Style Separation with LoRAs:**
  * **Objective:** To apply a character LoRA to define a subject's identity while using the base model's or a different LoRA's style.
  * **Method:** Use the LoraLoaderBlockWeight node. For the character LoRA, apply a block weight vector that emphasizes the semantic tier while de-emphasizing the stylistic tier. A starting point could be a vector where blocks 8-19 have a weight of 1.0, while blocks 20-40 have a weight of 0.0. To apply a separate style LoRA, chain another LoraLoaderBlockWeight node and apply its weights primarily to the later blocks (e.g., 20-40). This effectively isolates the application of each LoRA to its corresponding functional tier.
* **Recipe 2: Maximizing Coherence and Detail:**
  * **Objective:** To produce the highest quality video, especially for complex scenes with human subjects.
  * **Method:** Combine CFG with Skip Layer Guidance. Add a SkipLayerGuidanceDiT node after the model loader. Set the double\_layers and single\_layers parameters to target block 9 (e.g., "9"). Based on community experience, a guidance scale of around 3.0 is effective.47 Use the
    start\_percent and end\_percent parameters to apply SLG during the middle phase of the denoising steps (e.g., start at 0.16 and end at 1.0 for a 30-step process), as this is when semantic coherence is most critical.38
* **Recipe 3: Controlled Conceptual Blending (Morphing):**
  * **Objective:** To create a video where one subject or concept smoothly transforms into another.
  * **Method:** This advanced technique requires a workflow that can alter conditioning mid-generation. One approach involves using two KSampler nodes. The first sampler runs for a portion of the total steps (e.g., 15 out of 30\) with the initial text prompt. The resulting noisy latent is then passed to a second KSampler, which is fed a *different* text prompt for the remaining steps. To make the transition more targeted, this could be combined with a custom node that injects the second prompt's embedding specifically into the cross-attention layers of the semantic blocks (8-19) for the second KSampler's run. This localizes the semantic shift to the model's conceptual core, prompting a more integrated transformation rather than a simple crossfade.

### **4.3 Recommendations for Future Experimentation**

The ability to manipulate individual transformer blocks opens up numerous avenues for further research and development. The following areas represent promising directions for future experimentation by the advanced practitioner community:

* **Temporal vs. Spatial Block Specialization:** While this report proposes a general functional hierarchy, it is likely that within this structure, certain blocks specialize further in either temporal (motion, across-frame) or spatial (detail, within-frame) processing. This could be investigated by training LoRAs on datasets that isolate these variables (e.g., a "camera pan" LoRA vs. a "film grain" LoRA) and analyzing their impact across different block weightings. Visualizing the attention maps from different blocks could also reveal whether they focus more on inter-frame or intra-frame relationships.48
* **Multi-LoRA Block-Level Merging:** Current methods for combining multiple LoRAs often involve simple weighted averaging, which can lead to interference. A more sophisticated approach would be to merge LoRAs at the block level. For example, one could programmatically construct a new, hybrid LoRA by taking the weights for the semantic blocks (8-19) from a character LoRA and combining them with the weights for the stylistic blocks (20-40) from an art style LoRA. This could create a single, efficient adapter that cleanly separates and applies the desired traits without conflict.
* **Dynamic Guidance and Weight Scheduling:** The parameters for block manipulation are typically static throughout the generation process. A more advanced technique would be to dynamically alter these parameters across the denoising timesteps. For instance, a workflow could be designed to apply a "composition" LoRA with high strength to the early blocks during the first few steps, then switch to a "character" LoRA on the middle blocks for the intermediate steps, and finally apply a "detail" LoRA to the late blocks for the final steps. This would mimic a human artist's workflow, establishing structure first before refining content and adding details, potentially leading to more coherent and controllable outputs.

## **Conclusion**

The WanVideo 2.1 model, particularly its 14-billion parameter variant, is not a monolithic black box. Its architecture, centered on a deep stack of 40 Diffusion Transformer blocks, exhibits a clear hierarchical functional specialization. This analysis, synthesizing architectural documentation with extensive empirical evidence from the user community, has established a functional map dividing the stack into three distinct tiers: an initial **Compositional** tier (blocks \~1-7) that defines scene layout and macro-motion, a crucial **Semantic** tier (blocks \~8-19) that solidifies subject identity and action, and a final **Stylistic/Detail** tier (blocks \~20-40) that applies high-frequency textures and aesthetic finish.

This functional stratification provides a robust framework for understanding and predicting the outcomes of block-level interventions. The direct injection of a text embedding into the semantic core at blocks 8 and 9 is predicted to cause a fundamental shift in the video's core subject or action, offering a powerful tool for conceptual morphing. Community-driven techniques like Skip Layer Guidance, which improves coherence by targeting the semantic checkpoint at block 9, and the disabling of later blocks to enhance LoRA fidelity, serve as powerful confirmations of this hierarchical model.

Ultimately, these findings empower the advanced practitioner to move beyond simple prompt engineering and engage in a more direct form of "model surgery." By selecting the appropriate intervention—be it embedding injection, guidance perturbation, or block weighting—and targeting the correct functional tier, users can exert an unprecedented level of granular control over the generative process. This enables the precise separation of style from subject, the arbitration of conflicts between base models and fine-tuned adapters, and the enhancement of video quality and coherence, transforming the act of video generation from a process of discovery into one of deliberate and predictable creation. The continued exploration of these low-level mechanics promises to unlock even more sophisticated methods for artistic expression and technical control in the evolving landscape of generative video.

#### **Works cited**

1. Wan Video research paper | PDF | Video | Artificial Intelligence \- Scribd, accessed July 26, 2025, [https://www.scribd.com/document/850154743/Wan-Video-research-paper](https://www.scribd.com/document/850154743/Wan-Video-research-paper)
2. Wan 2.1 The Latest in Video Generative Models \- DigitalOcean, accessed July 26, 2025, [https://www.digitalocean.com/community/tutorials/wan-video-foundation-models](https://www.digitalocean.com/community/tutorials/wan-video-foundation-models)
3. Wan-AI/Wan2.1-T2V-14B \- Hugging Face, accessed July 26, 2025, [https://huggingface.co/Wan-AI/Wan2.1-T2V-14B](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B)
4. ArXiv Dives \- Diffusion Transformers \- Oxen.ai, accessed July 26, 2025, [https://www.oxen.ai/blog/arxiv-dives-diffusion-transformers](https://www.oxen.ai/blog/arxiv-dives-diffusion-transformers)
5. The Wan2.1 open-source video generative model competes Sora ..., accessed July 26, 2025, [https://www.mlwires.com/the-wan2-1-open-source-video-generative-model-competes-sora/](https://www.mlwires.com/the-wan2-1-open-source-video-generative-model-competes-sora/)
6. Wan 2.1 T2V 14B | Open Laboratory, accessed July 26, 2025, [https://openlaboratory.ai/models/wan2\_1-t2v-14b](https://openlaboratory.ai/models/wan2_1-t2v-14b)
7. Wan-AI/Wan2.1-T2V-1.3B \- Hugging Face, accessed July 26, 2025, [https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B)
8. Wan 2.1: The AI Video Revolution That's Out of This World (But Still Runs on Your Computer) \- DEV Community, accessed July 26, 2025, [https://dev.to/sayed\_ali\_alkamel/wan-21-the-ai-video-revolution-thats-out-of-this-world-but-still-runs-on-your-computer-2p4e](https://dev.to/sayed_ali_alkamel/wan-21-the-ai-video-revolution-thats-out-of-this-world-but-still-runs-on-your-computer-2p4e)
9. Tech Report \- Wan AI, accessed July 26, 2025, [https://www.wan-ai.org/about](https://www.wan-ai.org/about)
10. Wan-Video/Wan2.1: Wan: Open and Advanced Large-Scale Video Generative Models \- GitHub, accessed July 26, 2025, [https://github.com/Wan-Video/Wan2.1](https://github.com/Wan-Video/Wan2.1)
11. Qwen's Wan 2.1: A Guide With Practical Examples \- DataCamp, accessed July 26, 2025, [https://www.datacamp.com/tutorial/wan-2-1](https://www.datacamp.com/tutorial/wan-2-1)
12. Wan2.1: Best AI Video generation model, beats OpenAI Sora | by Mehul Gupta \- Medium, accessed July 26, 2025, [https://medium.com/data-science-in-your-pocket/wan2-1-best-open-sourced-ai-video-generation-model-beats-openai-sora-6ea081cbb8f8](https://medium.com/data-science-in-your-pocket/wan2-1-best-open-sourced-ai-video-generation-model-beats-openai-sora-6ea081cbb8f8)
13. Wan AI | Wan 2.1: Leading AI Video Generation Model, accessed July 26, 2025, [https://wan.video/welcome](https://wan.video/welcome)
14. How to replace WanVideo TextEncoder with ComfyUI's native CLIP Text Encode (Prompt) in wan2.1 workflow? \- Stack Overflow, accessed July 26, 2025, [https://stackoverflow.com/questions/79652561/how-to-replace-wanvideo-textencoder-with-comfyuis-native-clip-text-encode-prom](https://stackoverflow.com/questions/79652561/how-to-replace-wanvideo-textencoder-with-comfyuis-native-clip-text-encode-prom)
15. WAN Video model launched : r/LocalLLaMA \- Reddit, accessed July 26, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan\_video\_model\_launched/](https://www.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/)
16. Introduction to Diffusion Models for Machine Learning | SuperAnnotate, accessed July 26, 2025, [https://www.superannotate.com/blog/diffusion-models](https://www.superannotate.com/blog/diffusion-models)
17. Diffusion models explained simply \- Sean Goedecke, accessed July 26, 2025, [https://www.seangoedecke.com/diffusion-models-explained/](https://www.seangoedecke.com/diffusion-models-explained/)
18. Discover why Wan 2.1 is the best AI video model right now. \- Learn Think Diffusion, accessed July 26, 2025, [https://learn.thinkdiffusion.com/discover-why-wan-2-1-is-the-best-ai-video-model-right-now/](https://learn.thinkdiffusion.com/discover-why-wan-2-1-is-the-best-ai-video-model-right-now/)
19. Wan 2.1: Leading AI Video Generation Model (Wanx 2.1)|Wan AI, accessed July 26, 2025, [https://www.wan-ai.org/](https://www.wan-ai.org/)
20. Odd OOM issue in WSL with 4070 16GB VRAM (But Not with ComfyUi Diffusion Model Loader) \#208 \- GitHub, accessed July 26, 2025, [https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/208](https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/208)
21. Wan2.1-T2V-14B-Diffusers \- PromptLayer, accessed July 26, 2025, [https://www.promptlayer.com/models/wan21-t2v-14b-diffusers](https://www.promptlayer.com/models/wan21-t2v-14b-diffusers)
22. Diffusion Transformer (DiT) Models: A Beginner's Guide \- Encord, accessed July 26, 2025, [https://encord.com/blog/diffusion-models-with-transformers/](https://encord.com/blog/diffusion-models-with-transformers/)
23. What is a Transformer Model? \- IBM, accessed July 26, 2025, [https://www.ibm.com/think/topics/transformer-model](https://www.ibm.com/think/topics/transformer-model)
24. What are Transformers in Artificial Intelligence? \- AWS, accessed July 26, 2025, [https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/](https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/)
25. Transformer (deep learning architecture) \- Wikipedia, accessed July 26, 2025, [https://en.wikipedia.org/wiki/Transformer\_(deep\_learning\_architecture)](https://en.wikipedia.org/wiki/Transformer_\(deep_learning_architecture\))
26. The Building blocks of Transformers | by Maddie Lupu \- Medium, accessed July 26, 2025, [https://medium.com/@madalina.lupu.d/the-building-blocks-of-transformers-2b46057c0eb1](https://medium.com/@madalina.lupu.d/the-building-blocks-of-transformers-2b46057c0eb1)
27. Scalable Diffusion Models with Transformers \- CVF Open Access, accessed July 26, 2025, [https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles\_Scalable\_Diffusion\_Models\_with\_Transformers\_ICCV\_2023\_paper.pdf](https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf)
28. Transformer Architectures for Diffusion Models \- ApX Machine Learning, accessed July 26, 2025, [https://apxml.com/courses/advanced-diffusion-architectures/chapter-3-transformer-diffusion-models](https://apxml.com/courses/advanced-diffusion-architectures/chapter-3-transformer-diffusion-models)
29. Stable Diffusion 3: Multimodal Diffusion Transformer Model Explained \- Encord, accessed July 26, 2025, [https://encord.com/blog/stable-diffusion-3-text-to-image-model/](https://encord.com/blog/stable-diffusion-3-text-to-image-model/)
30. LoRA \- Hugging Face, accessed July 26, 2025, [https://huggingface.co/docs/peft/main/conceptual\_guides/lora](https://huggingface.co/docs/peft/main/conceptual_guides/lora)
31. A Friendly (but Thorough) Breakdown of the LoRA Paper | by Zaynab Awofeso \- Medium, accessed July 26, 2025, [https://medium.com/codex/a-friendly-but-thorough-breakdown-of-the-lora-paper-b77c6afa6e6f](https://medium.com/codex/a-friendly-but-thorough-breakdown-of-the-lora-paper-b77c6afa6e6f)
32. Lora Loader (Block Weight) \- RunComfy, accessed July 26, 2025, [https://www.runcomfy.com/comfyui-nodes/ComfyUI-Inspire-Pack/LoraLoaderBlockWeight-\_\_Inspire](https://www.runcomfy.com/comfyui-nodes/ComfyUI-Inspire-Pack/LoraLoaderBlockWeight-__Inspire)
33. ComfyUI Inspire Pack detailed guide \- RunComfy, accessed July 26, 2025, [https://www.runcomfy.com/comfyui-nodes/ComfyUI-Inspire-Pack](https://www.runcomfy.com/comfyui-nodes/ComfyUI-Inspire-Pack)
34. Everyone was asking me to upload an example, so here it is: SFW quality difference in Wan2.1 when disabling blocks 20-\>39 vs. using them (first is default, second disabled, follow by preview pictures) Lora strength \= 1, 800x800 49 frames pingpong : r/StableDiffusion \- Reddit, accessed July 26, 2025, [https://www.reddit.com/r/StableDiffusion/comments/1j84upk/everyone\_was\_asking\_me\_to\_upload\_an\_example\_so/](https://www.reddit.com/r/StableDiffusion/comments/1j84upk/everyone_was_asking_me_to_upload_an_example_so/)
35. LORA Block weight for Flux (inspire pack in comfyui) : face : r ..., accessed July 26, 2025, [https://www.reddit.com/r/StableDiffusion/comments/1f6aea3/lora\_block\_weight\_for\_flux\_inspire\_pack\_in/](https://www.reddit.com/r/StableDiffusion/comments/1f6aea3/lora_block_weight_for_flux_inspire_pack_in/)
36. Disabling blocks 20-\>39 really improved my video quality with LORA in Wan2.1 (Kijai) \- Reddit, accessed July 26, 2025, [https://www.reddit.com/r/StableDiffusion/comments/1j7rbmd/disabling\_blocks\_2039\_really\_improved\_my\_video/](https://www.reddit.com/r/StableDiffusion/comments/1j7rbmd/disabling_blocks_2039_really_improved_my_video/)
37. Dramatically enhance the quality of Wan 2.1 using skip layer guidance : r/StableDiffusion, accessed July 26, 2025, [https://www.reddit.com/r/StableDiffusion/comments/1jac3wm/dramatically\_enhance\_the\_quality\_of\_wan\_21\_using/](https://www.reddit.com/r/StableDiffusion/comments/1jac3wm/dramatically_enhance_the_quality_of_wan_21_using/)
38. Question about Skip Layer Guidance on Wan video : r/StableDiffusion \- Reddit, accessed July 26, 2025, [https://www.reddit.com/r/StableDiffusion/comments/1k3tudz/question\_about\_skip\_layer\_guidance\_on\_wan\_video/](https://www.reddit.com/r/StableDiffusion/comments/1k3tudz/question_about_skip_layer_guidance_on_wan_video/)
39. \[2411.18664\] Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling \- arXiv, accessed July 26, 2025, [https://arxiv.org/abs/2411.18664](https://arxiv.org/abs/2411.18664)
40. \[2503.20314\] Wan: Open and Advanced Large-Scale Video Generative Models \- arXiv, accessed July 26, 2025, [https://arxiv.org/abs/2503.20314](https://arxiv.org/abs/2503.20314)
41. Wan: Open and Advanced Large-Scale Video Generative Models \- ResearchGate, accessed July 26, 2025, [https://www.researchgate.net/publication/390991684\_Wan\_Open\_and\_Advanced\_Large-Scale\_Video\_Generative\_Models](https://www.researchgate.net/publication/390991684_Wan_Open_and_Advanced_Large-Scale_Video_Generative_Models)
42. kijai/ComfyUI-WanVideoWrapper \- GitHub, accessed July 26, 2025, [https://github.com/kijai/ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
43. The MOST Impressive Video AI | Wan2.1 ComfyUI Complete Guide \- YouTube, accessed July 26, 2025, [https://www.youtube.com/watch?v=gdoRVlJH218\&pp=0gcJCdgAo7VqN5tD](https://www.youtube.com/watch?v=gdoRVlJH218&pp=0gcJCdgAo7VqN5tD)
44. ComfyUI Nodes Info, accessed July 26, 2025, [https://ltdrdata.github.io/](https://ltdrdata.github.io/)
45. ComfyUI Node: Lora Block Info \- RunComfy, accessed July 26, 2025, [https://www.runcomfy.com/comfyui-nodes/ComfyUI-Inspire-Pack/LoraBlockInfo-\_\_Inspire](https://www.runcomfy.com/comfyui-nodes/ComfyUI-Inspire-Pack/LoraBlockInfo-__Inspire)
46. LoRA Block Info \- ComfyUI Cloud, accessed July 26, 2025, [https://comfy.icu/node/LoraBlockInfo-Inspire](https://comfy.icu/node/LoraBlockInfo-Inspire)
47. SkipLayerGuidanceDiT \- RunComfy, accessed July 26, 2025, [https://www.runcomfy.com/comfyui-nodes/ComfyUI/skip-layer-guidance-di-t](https://www.runcomfy.com/comfyui-nodes/ComfyUI/skip-layer-guidance-di-t)
48. Understanding Attention Mechanism in Video Diffusion Models \- arXiv, accessed July 26, 2025, [https://arxiv.org/html/2504.12027v2](https://arxiv.org/html/2504.12027v2)
49. Understanding Attention Mechanism in Video Diffusion Models \- ResearchGate, accessed July 26, 2025, [https://www.researchgate.net/publication/390845961\_Understanding\_Attention\_Mechanism\_in\_Video\_Diffusion\_Models](https://www.researchgate.net/publication/390845961_Understanding_Attention_Mechanism_in_Video_Diffusion_Models)
50. Analysis of Attention in Video Diffusion Transformers \- arXiv, accessed July 26, 2025, [https://arxiv.org/html/2504.10317v1](https://arxiv.org/html/2504.10317v1)
